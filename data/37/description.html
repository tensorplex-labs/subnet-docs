<p>
  <strong>Aurelius</strong> is a decentralized protocol for surfacing and
  verifying alignment failures in large language models. It transforms
  adversarial prompts, model outputs, scoring artifacts, and interpretability
  data into structured, reproducible datasets — all without relying on
  centralized oversight.
</p>
<p>
  Built on the <strong>Bittensor</strong> network, Aurelius incentivizes a
  peer-to-peer ecosystem of adversarial prompters (<strong>miners</strong>),
  independent auditors (<strong>validators</strong>), and a dynamic rules layer
  known as the <strong>Tribunate</strong>. Together, these agents generate
  alignment pressure through contestation, not consensus — creating artifacts
  that can be used to train, fine-tune, or audit models in a reproducible and
  interpretable way.
</p>
<h2>Why This Matters</h2>
<p>
  Modern AI systems often appear safe on the surface, but
  <strong>fail to reason honestly under pressure</strong>. Existing alignment
  methods rely heavily on centralized oversight, fixed reward models, and
  shallow behavioral signals — suppressing disagreement and failing to reveal
  model internals. This leads to <strong>alignment faking</strong>, brittle
  safety filters, and unverifiable outputs.
</p>
<p>
  Aurelius challenges this paradigm by enabling
  <strong>any motivated agent</strong> to expose failure, verify it
  independently, and turn it into usable data — all while preserving reasoning,
  scoring methods, and provenance through cryptographic commitments.
</p>
<h2>What Aurelius Offers</h2>
<ul>
  <li>
    A <strong>reproducible pipeline</strong> for surfacing misalignment under
    adversarial conditions
  </li>
  <li>
    A <strong>decentralized scoring system</strong> that incentivizes
    independent validators
  </li>
  <li>
    A way to <strong>capture reasoning traces</strong> and
    <strong>mechanistic interpretability artifacts</strong>
  </li>
  <li>
    An open, evolving dataset for
    <strong>training safer and more honest models</strong>
  </li>
  <li>
    A philosophical foundation rooted in
    <strong>structured disagreement</strong> and
    <strong>epistemic alignment</strong>
  </li>
</ul>
<h2>Who It's For</h2>
<ul>
  <li>
    <strong>Model creators</strong> seeking reproducible failure data and
    external alignment pressure
  </li>
  <li>
    <strong>Researchers</strong> interested in adversarial prompting,
    interpretability, and Chain-of-Thought
  </li>
  <li>
    <strong>Auditors and tool builders</strong> who want real-world examples of
    model failures
  </li>
  <li>
    <strong>Red-teamers</strong> looking to be rewarded for high-signal
    discoveries
  </li>
</ul>
